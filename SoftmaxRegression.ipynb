{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEqbi7+ATLoSFj1vLFXbQS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"H2A9eFzhRkal"},"outputs":[],"source":["import numpy as np\n","import matplotlib as plt\n","import pandas as pd"]},{"cell_type":"code","source":["class SoftmaxRegression():\n","  def __init__(self, n_epochs=100,lr=0.1):\n","    self.n_epochs, self.lr = n_epochs, lr\n","\n","  def softmax(self, X):\n","    z = np.dot(X , self.weights.T) + self.bias\n","    pk = np.exp(z) / np.sum(np.exp(np.array(z)), axis=1, keepdims=True)\n","    return pk\n","\n","  def predict_proba(self, X):\n","    return self.softmax(X)\n","\n","  def predict(self, X):\n","    return np.argmax(self.softmax(X), axis=1)\n","\n","  def initialize_params(self, X, y):\n","    weights = np.random.randn(y.shape[1], X.shape[1]) * 0.01\n","    bias_shape = (1, y.shape[1])\n","    biases = np.zeros(bias_shape)\n","    return weights, biases\n","\n","  def one_hot(self, y):\n","    n_rows = y.shape[0]\n","    n_class = len(y.unique())\n","    one_hot = np.zeros((n_rows, n_class))\n","    one_hot[np.arange(len(y)), y] = 1\n","    return one_hot\n","\n","  def cross_entropy(self, X, y):\n","    pk = self.softmax(X)\n","    loss = y * np.log(pk)\n","    return np.sum(loss) / -X.shape[0]\n","\n","  def fit(self, X, y):\n","    m = X.shape[0]\n","    y = self.one_hot(y)\n","    self.weights, self.bias = self.initialize_params(X,y)\n","    for epoch in range(self.n_epochs):\n","      pk = self.softmax(X)\n","      w_gradient = np.dot((pk-y).T , X) / m\n","      self.weights -= self.lr * w_gradient\n","      b_gradient = np.sum((pk-y), axis=0) / m\n","      self.bias -= self.lr * b_gradient.ravel()\n","\n","      if epoch % 100 == 0:\n","        print(f\"epoch num {epoch}, Loss = \",self.cross_entropy(X,y))"],"metadata":{"id":"dPP8ni3PUUwb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","iris = load_iris(as_frame=True)\n","X = iris.data\n","y = iris.target"],"metadata":{"id":"LiTsYcyhb01z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["our_model = SoftmaxRegression(n_epochs=1000)\n","our_model.fit(X, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8ooBJXabQdk","executionInfo":{"status":"ok","timestamp":1715799411170,"user_tz":-180,"elapsed":314,"user":{"displayName":"Omar Tarek","userId":"17063106296441777024"}},"outputId":"069e7dfd-c7bc-4172-cb88-69c5d55a1d2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch num 0, Loss =  1.0254290549358605\n","epoch num 100, Loss =  0.46841623458536846\n","epoch num 200, Loss =  0.26147596377133964\n","epoch num 300, Loss =  0.21850473275236346\n","epoch num 400, Loss =  0.19122980773985815\n","epoch num 500, Loss =  0.17224595636613682\n","epoch num 600, Loss =  0.15821936676537465\n","epoch num 700, Loss =  0.14739769415145312\n","epoch num 800, Loss =  0.13877108142112551\n","epoch num 900, Loss =  0.13171621812269793\n"]}]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss\n","\n","# Create and train the model\n","model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n","model.fit(X, y)\n","\n","# Make predictions\n","y_pred_proba = model.predict_proba(X)\n","our_preds = our_model.predict_proba(X)\n","\n","\n","# Calculate cross-entropy loss\n","loss = log_loss(y, y_pred_proba)\n","our_loss = log_loss(y, our_preds)\n","\n","# Print cross-entropy loss\n","print(f'Cross-Entropy Loss Sklearn Model: {loss}')\n","print(f'Cross-Entropy Loss For Our Model: {our_loss}')\n","\n","# Print the weights and bias\n","# print(f'Weights: {model.coef_}')\n","# print(f'Bias: {model.intercept_}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qy4ukyLLcl8r","executionInfo":{"status":"ok","timestamp":1715799413177,"user_tz":-180,"elapsed":304,"user":{"displayName":"Omar Tarek","userId":"17063106296441777024"}},"outputId":"5305faa1-b792-4b33-9c7c-6764ee44401c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-Entropy Loss Sklearn Model: 0.11963578904962134\n","Cross-Entropy Loss For Our Model: 0.12588131421695473\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"J8cnhyl8mkNh"},"execution_count":null,"outputs":[]}]}